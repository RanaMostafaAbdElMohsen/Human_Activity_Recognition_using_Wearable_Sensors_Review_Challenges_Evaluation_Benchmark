{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OpportunityUCIDataset.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPFuu1bLKLYNW1PGOm3lw8x"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"F3GYNEuUjLGV","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import time\n","import random\n","import zipfile\n","import argparse\n","import numpy as np\n","import pickle as cp\n","\n","from io import BytesIO\n","from pandas import Series\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"awgYS4JMjQaS","colab_type":"code","outputId":"ee5a3f6e-81dd-44a6-937b-0a597cebcd17","executionInfo":{"status":"ok","timestamp":1585584846249,"user_tz":-120,"elapsed":426,"user":{"displayName":"brain pinky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSJ-MfHyLlQ17ipaf3IqBsqmMEcsDxuSrwhgT_Ng=s64","userId":"06438107502629971890"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"66LjSy_9qTA-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvmAC3cijR8-","colab_type":"code","outputId":"e70b0ba2-a660-4e70-bb83-0b419bd6f404","executionInfo":{"status":"ok","timestamp":1585508452731,"user_tz":-120,"elapsed":26682,"user":{"displayName":"brain pinky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSJ-MfHyLlQ17ipaf3IqBsqmMEcsDxuSrwhgT_Ng=s64","userId":"06438107502629971890"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!unzip '/content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset.zip' -d '/content/drive/My Drive/Computer_vision_project/data/Opportunity/'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset.zip\n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/\n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/\n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/column_names.txt  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/label_legend.txt  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S1-ADL1.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S1-ADL2.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S1-ADL3.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S1-ADL4.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S1-ADL5.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S1-Drill.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S2-ADL1.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S2-ADL2.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S2-ADL3.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S2-ADL4.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S2-ADL5.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S2-Drill.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S3-ADL1.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S3-ADL2.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S3-ADL3.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S3-ADL4.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S3-ADL5.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S3-Drill.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S4-ADL1.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S4-ADL2.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S4-ADL3.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S4-ADL4.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S4-ADL5.dat  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/dataset/S4-Drill.dat  \n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/\n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/ActivityRecognitionBaselines.pdf  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/Chavarriaga - The Opportunity challenge-A benchmark database for on-body sensor-based activity recognition (PRL, 2013).pdf  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/dataset_statistics.pdf  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/documentation.html  \n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/\n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/arena3.png  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/bt_obj1.jpg  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/Fig1Cl.png  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/Fig1Cr.png  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/label.png  \n"," extracting: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/logos-opportunity-final_50p.png  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/logos-opportunity-final_50p_challenge.png  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/motion_jacket_complete.jpg  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/objects.JPG  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/reed.jpg  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/Reed_Switch_Configuration.png  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/img/usb_sensor.jpg  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/locomotion_instances.tex  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/mlgesture_instances.tex  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/OPPORTUNITY_D5.1.pdf  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/doc/Roggen - Collecting complex activity datasets in highly rich networked sensor environments (INSS, 2010).pdf  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/README  \n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/\n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/\n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/classifiers/\n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/classifiers/classifyAndReject.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/classifiers/GausianClassify.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/classifiers/knn.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/classifiers/nccClassify.m  \n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/data/\n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/features/\n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/features/expandingLabels.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/features/featureExtraction.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/features/featureReduction.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/features/missingValueHandler.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/features/movtimavg.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/features/windowingLabels.m  \n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/\n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/AreaUnderROC.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/clsAccuracy.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/measures.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/rocCurves.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/separate.m  \n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/ward/\n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/wardbars.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/ward/a-categorisation-of-performance-errors-in-continuous-context-recognition-ward-2005.pdf  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/ward/mset.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/ward/mset_segments.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/ward/plot_mset_errors.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/ward/README  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/ward/read_seg_info.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/ward/test_mset_plotting.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/ward/wardbars.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/measures/ward/write_seg_info.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/parameters.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/prepareData.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/readme.txt  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/res2mat.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/RunBenchmarking.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/tanalyze.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/benchmark/tarrange.m  \n","   creating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/DataExplorer/\n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/DataExplorer/label_panorama.fig  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/DataExplorer/label_panorama.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/DataExplorer/label_panorama_2009.fig  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/DataExplorer/label_panorama_2009.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/DataExplorer/readme.txt  \n"," extracting: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/DataExplorer/root.txt  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/DataExplorer/signal_scope_Callback.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/DataExplorer/signal_scroll_Callback.m  \n","  inflating: /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset/scripts/DataExplorer/underscore_clean.m  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iVeC9xhqjiPc","colab_type":"code","colab":{}},"source":["NB_SENSOR_CHANNELS = 113"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IoOFiaT2kNJO","colab_type":"code","colab":{}},"source":["# Hardcoded names of the files defining the OPPORTUNITY challenge data. As named in the original data.\n","OPPORTUNITY_DATA_FILES = ['OpportunityUCIDataset/dataset/S1-Drill.dat',\n","                          'OpportunityUCIDataset/dataset/S1-ADL1.dat',\n","                          'OpportunityUCIDataset/dataset/S1-ADL2.dat',\n","                          'OpportunityUCIDataset/dataset/S1-ADL3.dat',\n","                          'OpportunityUCIDataset/dataset/S1-ADL4.dat',\n","                          'OpportunityUCIDataset/dataset/S1-ADL5.dat',\n","                          'OpportunityUCIDataset/dataset/S2-Drill.dat',\n","                          'OpportunityUCIDataset/dataset/S2-ADL1.dat',\n","                          'OpportunityUCIDataset/dataset/S2-ADL2.dat',\n","                          'OpportunityUCIDataset/dataset/S2-ADL3.dat',\n","                          'OpportunityUCIDataset/dataset/S3-Drill.dat',\n","                          'OpportunityUCIDataset/dataset/S3-ADL1.dat',\n","                          'OpportunityUCIDataset/dataset/S3-ADL2.dat',\n","                          'OpportunityUCIDataset/dataset/S3-ADL3.dat',\n","                          'OpportunityUCIDataset/dataset/S2-ADL4.dat',\n","                          'OpportunityUCIDataset/dataset/S2-ADL5.dat',\n","                          'OpportunityUCIDataset/dataset/S3-ADL4.dat',\n","                          'OpportunityUCIDataset/dataset/S3-ADL5.dat'\n","                          ]\n","\n","# Hardcoded thresholds to define global maximums and minimums for every one of the 113 sensor channels employed in the\n","# OPPORTUNITY challenge\n","NORM_MAX_THRESHOLDS = [3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n","                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n","                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n","                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n","                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n","                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n","                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n","                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n","                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n","                       250,    25,     200,    5000,   5000,   5000,   5000,   5000,   5000,\n","                       10000,  10000,  10000,  10000,  10000,  10000,  250,    250,    25,\n","                       200,    5000,   5000,   5000,   5000,   5000,   5000,   10000,  10000,\n","                       10000,  10000,  10000,  10000,  250, ]\n","\n","NORM_MIN_THRESHOLDS = [-3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n","                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n","                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n","                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n","                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n","                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n","                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n","                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n","                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n","                       -250,   -100,   -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,\n","                       -10000, -10000, -10000, -10000, -10000, -10000, -250,   -250,   -100,\n","                       -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,  -10000, -10000,\n","                       -10000, -10000, -10000, -10000, -250, ]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"85kW2NvXkST4","colab_type":"code","colab":{}},"source":["def select_columns_opp(data):\n","    \"\"\"Selection of the 113 columns employed in the OPPORTUNITY challenge\n","    :param data: numpy integer matrix\n","        Sensor data (all features)\n","    :return: numpy integer matrix\n","        Selection of features\n","    \"\"\"\n","\n","    #                     included-excluded\n","    features_delete = np.arange(46, 50)\n","    features_delete = np.concatenate([features_delete, np.arange(59, 63)])\n","    features_delete = np.concatenate([features_delete, np.arange(72, 76)])\n","    features_delete = np.concatenate([features_delete, np.arange(85, 89)])\n","    features_delete = np.concatenate([features_delete, np.arange(98, 102)])\n","    features_delete = np.concatenate([features_delete, np.arange(134, 243)])\n","    features_delete = np.concatenate([features_delete, np.arange(244, 249)])\n","    return np.delete(data, features_delete, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PVzY5LpmkWXI","colab_type":"code","colab":{}},"source":["def normalize(data, max_list, min_list):\n","    \"\"\"Normalizes all sensor channels\n","    :param data: numpy integer matrix\n","        Sensor data\n","    :param max_list: numpy integer array\n","        Array containing maximums values for every one of the 113 sensor channels\n","    :param min_list: numpy integer array\n","        Array containing minimum values for every one of the 113 sensor channels\n","    :return:\n","        Normalized sensor data\n","    \"\"\"\n","    max_list, min_list = np.array(max_list), np.array(min_list)\n","    diffs = max_list - min_list\n","    for i in np.arange(data.shape[1]):\n","        data[:, i] = (data[:, i]-min_list[i])/diffs[i]\n","    #     Checking the boundaries\n","    data[data > 1] = 0.99\n","    data[data < 0] = 0.00\n","    return data\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L4gaKF0YkY6J","colab_type":"code","colab":{}},"source":["def divide_x_y(data, label):\n","    \"\"\"Segments each sample into features and label\n","    :param data: numpy integer matrix\n","        Sensor data\n","    :param label: string, ['gestures' (default), 'locomotion']\n","        Type of activities to be recognized\n","    :return: numpy integer matrix, numpy integer array\n","        Features encapsulated into a matrix and labels as an array\n","    \"\"\"\n","\n","    data_x = data[:, 1:114]\n","    if label not in ['locomotion', 'gestures']:\n","            raise RuntimeError(\"Invalid label: '%s'\" % label)\n","    if label == 'locomotion':\n","        data_y = data[:, 114]  # Locomotion label\n","    elif label == 'gestures':\n","        data_y = data[:, 115]  # Gestures label\n","\n","    return data_x, data_y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cpoEZEW5kcSp","colab_type":"code","colab":{}},"source":["def adjust_idx_labels(data_y, label):\n","    \"\"\"Transforms original labels into the range [0, nb_labels-1]\n","    :param data_y: numpy integer array\n","        Sensor labels\n","    :param label: string, ['gestures' (default), 'locomotion']\n","        Type of activities to be recognized\n","    :return: numpy integer array\n","        Modified sensor labels\n","    \"\"\"\n","\n","    if label == 'locomotion':  # Labels for locomotion are adjusted\n","        data_y[data_y == 4] = 3\n","        data_y[data_y == 5] = 4\n","    elif label == 'gestures':  # Labels for gestures are adjusted\n","        data_y[data_y == 406516] = 1\n","        data_y[data_y == 406517] = 2\n","        data_y[data_y == 404516] = 3\n","        data_y[data_y == 404517] = 4\n","        data_y[data_y == 406520] = 5\n","        data_y[data_y == 404520] = 6\n","        data_y[data_y == 406505] = 7\n","        data_y[data_y == 404505] = 8\n","        data_y[data_y == 406519] = 9\n","        data_y[data_y == 404519] = 10\n","        data_y[data_y == 406511] = 11\n","        data_y[data_y == 404511] = 12\n","        data_y[data_y == 406508] = 13\n","        data_y[data_y == 404508] = 14\n","        data_y[data_y == 408512] = 15\n","        data_y[data_y == 407521] = 16\n","        data_y[data_y == 405506] = 17\n","    return data_y\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FA6x_g6zl72N","colab_type":"code","colab":{}},"source":["def check_data(data_set):\n","    \"\"\"Try to access to the file and checks if dataset is in the data directory\n","       In case the file is not found try to download it from original location\n","    :param data_set:\n","            Path with original OPPORTUNITY zip file\n","    :return:\n","    \"\"\"\n","    print ('Checking dataset {0}'.format(data_set))\n","    data_dir, data_file = os.path.split(data_set)\n","    # When a directory is not provided, check if dataset is in the data directory\n","    if data_dir == \"\" and not os.path.isfile(data_set):\n","        new_path = os.path.join(os.path.split(__file__)[0], \"data\", data_set)\n","        if os.path.isfile(new_path) or data_file == 'OpportunityUCIDataset.zip':\n","            data_set = new_path\n","\n","    # When dataset not found, try to download it from UCI repository\n","    if (not os.path.isfile(data_set)) and data_file == 'OpportunityUCIDataset.zip':\n","        print ('... dataset path {0} not found'.format(data_set))\n","        import urllib\n","        origin = (\n","            'https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip'\n","        )\n","        if not os.path.exists(data_dir):\n","            print ('... creating directory {0}'.format(data_dir))\n","            os.makedirs(data_dir)\n","        print ('... downloading data from {0}'.format(origin))\n","        urllib.urlretrieve(origin, data_set)\n","\n","    return data_dir\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVriZoyXmEhK","colab_type":"code","colab":{}},"source":["def process_dataset_file(data, label):\n","    \"\"\"Function defined as a pipeline to process individual OPPORTUNITY files\n","    :param data: numpy integer matrix\n","        Matrix containing data samples (rows) for every sensor channel (column)\n","    :param label: string, ['gestures' (default), 'locomotion']\n","        Type of activities to be recognized\n","    :return: numpy integer matrix, numy integer array\n","        Processed sensor data, segmented into features (x) and labels (y)\n","    \"\"\"\n","\n","    # Select correct columns\n","    data = select_columns_opp(data)\n","\n","    # Colums are segmentd into features and labels\n","    data_x, data_y =  divide_x_y(data, label)\n","    data_y = adjust_idx_labels(data_y, label)\n","    data_y = data_y.astype(int)\n","\n","    # Perform linear interpolation\n","    data_x = np.array([Series(i).interpolate() for i in data_x.T]).T\n","\n","    # Remaining missing data are converted to zero\n","    data_x[np.isnan(data_x)] = 0\n","\n","    # All sensor channels are normalized\n","    data_x = normalize(data_x, NORM_MAX_THRESHOLDS, NORM_MIN_THRESHOLDS)\n","\n","    return data_x, data_y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MGK3icVomJdB","colab_type":"code","colab":{}},"source":["def generate_data(dataset, target_filename, label):\n","    \"\"\"Function to read the OPPORTUNITY challenge raw data and process all sensor channels\n","    :param dataset: string\n","        Path with original OPPORTUNITY zip file\n","    :param target_filename: string\n","        Processed file\n","    :param label: string, ['gestures' (default), 'locomotion']\n","        Type of activities to be recognized. The OPPORTUNITY dataset includes several annotations to perform\n","        recognition modes of locomotion/postures and recognition of sporadic gestures.\n","    \"\"\"\n","\n","    data_dir = check_data(dataset)\n","\n","    data_x = np.empty((0, NB_SENSOR_CHANNELS))\n","    data_y = np.empty((0))\n","\n","    zf = zipfile.ZipFile(dataset)\n","    print ('Processing dataset files ...')\n","    for filename in OPPORTUNITY_DATA_FILES:\n","        try:\n","            data = np.loadtxt(BytesIO(zf.read(filename)))\n","            print ('... file {0}'.format(filename))\n","            x, y = process_dataset_file(data, label)\n","            data_x = np.vstack((data_x, x))\n","            data_y = np.concatenate([data_y, y])\n","        except KeyError:\n","            print ('ERROR: Did not find {0} in zip file'.format(filename))\n","\n","    # Dataset is segmented into train and test\n","    nb_training_samples = 557963\n","    # The first 18 OPPORTUNITY data files define the traning dataset, comprising 557963 samples\n","    X_train, y_train = data_x[:nb_training_samples,:], data_y[:nb_training_samples]\n","    X_test, y_test = data_x[nb_training_samples:,:], data_y[nb_training_samples:]\n","\n","    print (\"Final datasets with size: | train {0} | test {1} | \".format(X_train.shape,X_test.shape))\n","\n","    obj = [(X_train, y_train), (X_test, y_test)]\n","    f = open(target_filename,'wb')\n","    cp.dump(obj, f, protocol=cp.HIGHEST_PROTOCOL)\n","    f.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESgVO1qxmdcN","colab_type":"code","outputId":"e7e180cf-0885-4653-94ac-a31cbfdfce99","executionInfo":{"status":"ok","timestamp":1585584992371,"user_tz":-120,"elapsed":118768,"user":{"displayName":"brain pinky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSJ-MfHyLlQ17ipaf3IqBsqmMEcsDxuSrwhgT_Ng=s64","userId":"06438107502629971890"}},"colab":{"base_uri":"https://localhost:8080/","height":381}},"source":["generate_data('/content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset.zip', '/content/drive/My Drive/Computer_vision_project/data/Opportunity/Data_set.txt', 'locomotion')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Checking dataset /content/drive/My Drive/Computer_vision_project/data/Opportunity/OpportunityUCIDataset.zip\n","Processing dataset files ...\n","... file OpportunityUCIDataset/dataset/S1-Drill.dat\n","... file OpportunityUCIDataset/dataset/S1-ADL1.dat\n","... file OpportunityUCIDataset/dataset/S1-ADL2.dat\n","... file OpportunityUCIDataset/dataset/S1-ADL3.dat\n","... file OpportunityUCIDataset/dataset/S1-ADL4.dat\n","... file OpportunityUCIDataset/dataset/S1-ADL5.dat\n","... file OpportunityUCIDataset/dataset/S2-Drill.dat\n","... file OpportunityUCIDataset/dataset/S2-ADL1.dat\n","... file OpportunityUCIDataset/dataset/S2-ADL2.dat\n","... file OpportunityUCIDataset/dataset/S2-ADL3.dat\n","... file OpportunityUCIDataset/dataset/S3-Drill.dat\n","... file OpportunityUCIDataset/dataset/S3-ADL1.dat\n","... file OpportunityUCIDataset/dataset/S3-ADL2.dat\n","... file OpportunityUCIDataset/dataset/S3-ADL3.dat\n","... file OpportunityUCIDataset/dataset/S2-ADL4.dat\n","... file OpportunityUCIDataset/dataset/S2-ADL5.dat\n","... file OpportunityUCIDataset/dataset/S3-ADL4.dat\n","... file OpportunityUCIDataset/dataset/S3-ADL5.dat\n","Final datasets with size: | train (557963, 113) | test (118750, 113) | \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"syJ1KRGhnKQx","colab_type":"code","colab":{}},"source":["NB_SENSOR_CHANNELS = 113\n","NB_SENSOR_CHANNELS_WITH_FILTERING = 149\n","\n","# Hardcoded number of classes in the gesture recognition problem\n","NUM_CLASSES = 5\n","\n","# Hardcoded length of the sliding window mechanism employed to segment the data\n","SLIDING_WINDOW_LENGTH = 24\n","\n","# Length of the input sequence after convolutional operations\n","FINAL_SEQUENCE_LENGTH = 8\n","\n","# Hardcoded step of the sliding window mechanism employed to segment the data\n","SLIDING_WINDOW_STEP = int(SLIDING_WINDOW_LENGTH/2)\n","SLIDING_WINDOW_STEP_SHORT = SLIDING_WINDOW_STEP\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rmS-xedDnMhT","colab_type":"code","colab":{}},"source":["def load_dataset(filename):\n","\n","    f = open(filename, 'rb')\n","    data = cp.load(f)\n","    f.close()\n","\n","    X_train, y_train = data[0]\n","    X_test, y_test = data[1]\n","\n","    print(\" ..from file {}\".format(filename))\n","    print(\" ..reading instances: train {0}, test {1}\".format(X_train.shape, X_test.shape))\n","\n","    X_train = X_train.astype(np.float32)\n","    X_test = X_test.astype(np.float32)\n","\n","    # The targets are casted to int8 for GPU compatibility.\n","    y_train = y_train.astype(np.uint8)\n","    y_test = y_test.astype(np.uint8)\n","\n","    return X_train, y_train, X_test, y_test\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GM78BbOunafZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","from numpy.lib.stride_tricks import as_strided as ast\n","\n","def norm_shape(shape):\n","    '''\n","    Normalize numpy array shapes so they're always expressed as a tuple,\n","    even for one-dimensional shapes.\n","    Parameters\n","        shape - an int, or a tuple of ints\n","    Returns\n","        a shape tuple\n","    '''\n","    try:\n","        i = int(shape)\n","        return (i,)\n","    except TypeError:\n","        # shape was not a number\n","        pass\n","\n","    try:\n","        t = tuple(shape)\n","        return t\n","    except TypeError:\n","        # shape was not iterable\n","        pass\n","\n","    raise TypeError('shape must be an int, or a tuple of ints')\n","\n","def sliding_window(a,ws,ss = None,flatten = True):\n","    '''\n","    Return a sliding window over a in any number of dimensions\n","    Parameters:\n","        a  - an n-dimensional numpy array\n","        ws - an int (a is 1D) or tuple (a is 2D or greater) representing the size\n","             of each dimension of the window\n","        ss - an int (a is 1D) or tuple (a is 2D or greater) representing the\n","             amount to slide the window in each dimension. If not specified, it\n","             defaults to ws.\n","        flatten - if True, all slices are flattened, otherwise, there is an\n","                  extra dimension for each dimension of the input.\n","    Returns\n","        an array containing each n-dimensional window from a\n","    '''\n","\n","    if None is ss:\n","        # ss was not provided. the windows will not overlap in any direction.\n","        ss = ws\n","    ws = norm_shape(ws)\n","    ss = norm_shape(ss)\n","\n","    # convert ws, ss, and a.shape to numpy arrays so that we can do math in every\n","    # dimension at once.\n","    ws = np.array(ws)\n","    ss = np.array(ss)\n","    shape = np.array(a.shape)\n","\n","\n","    # ensure that ws, ss, and a.shape all have the same number of dimensions\n","    ls = [len(shape),len(ws),len(ss)]\n","    if 1 != len(set(ls)):\n","        raise ValueError(\\\n","        'a.shape, ws and ss must all have the same length. They were %s' % str(ls))\n","\n","    # ensure that ws is smaller than a in every dimension\n","    if np.any(ws > shape):\n","        raise ValueError(\\\n","        'ws cannot be larger than a in any dimension.\\\n"," a.shape was %s and ws was %s' % (str(a.shape),str(ws)))\n","\n","    # how many slices will there be in each dimension?\n","    newshape = norm_shape(((shape - ws) // ss) + 1)\n","    # the shape of the strided array will be the number of slices in each dimension\n","    # plus the shape of the window (tuple addition)\n","    newshape += norm_shape(ws)\n","    # the strides tuple will be the array's strides multiplied by step size, plus\n","    # the array's strides (tuple addition)\n","    newstrides = norm_shape(np.array(a.strides) * ss) + a.strides\n","    strided = ast(a,shape = newshape,strides = newstrides)\n","    if not flatten:\n","        return strided\n","\n","    # Collapse strided so that it has one more dimension than the window.  I.e.,\n","    # the new array is a flat list of slices.\n","    meat = len(ws) if ws.shape else 0\n","    firstdim = (np.product(newshape[:-meat]),) if ws.shape else ()\n","    dim = firstdim + (newshape[-meat:])\n","    # remove any dimensions with size 1\n","    \n","    dim = filter(lambda i : i != 1,dim)\n","    # print('dim',list(dim))\n","    dimm_=list(dim)\n","    \n","    # print('strided',strided)\n","    print(dimm_)\n","    return strided.reshape(tuple(dimm_))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FbphY2QbnRGv","colab_type":"code","outputId":"e65508da-133c-4469-9779-c06adca2b244","executionInfo":{"status":"ok","timestamp":1585584995224,"user_tz":-120,"elapsed":1855,"user":{"displayName":"brain pinky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSJ-MfHyLlQ17ipaf3IqBsqmMEcsDxuSrwhgT_Ng=s64","userId":"06438107502629971890"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["print(\"Loading data...\")\n","X_train, y_train, X_test, y_test = load_dataset('/content/drive/My Drive/Computer_vision_project/data/Opportunity/Data_set.txt')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Loading data...\n"," ..from file /content/drive/My Drive/Computer_vision_project/data/Opportunity/Data_set.txt\n"," ..reading instances: train (557963, 113), test (118750, 113)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SNHyEmFJd-je","colab_type":"code","colab":{}},"source":["def one_hot(y):\n","    \"\"\"convert label from dense to one hot\n","      argument:\n","        label: ndarray dense label ,shape: [sample_num,1]\n","      return:\n","        one_hot_label: ndarray  one hot, shape: [sample_num,n_class]\n","    \"\"\"\n","    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n","\n","    y = y.reshape(len(y))\n","    n_values = np.max(y) + 1\n","    return np.eye(n_values)[np.array(y, dtype=np.int32)]  # Returns FLOATS"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FdXbOxcnUDm","colab_type":"code","colab":{}},"source":["assert (NB_SENSOR_CHANNELS_WITH_FILTERING == X_train.shape[1] or NB_SENSOR_CHANNELS == X_train.shape[1])\n","\n","def opp_sliding_window(data_x, data_y, ws, ss):\n","    data_x = sliding_window(data_x,(ws,data_x.shape[1]),(ss,1))\n","    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y,ws,ss)])\n","    data_x,data_y = data_x.astype(np.float32),one_hot(data_y.reshape(len(data_y)).astype(np.uint8))\n","    # print(\" ..after sliding window (testing): inputs {0}, targets {1}\".format(X_test.shape, y_test.shape))\n","    return data_x, data_y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIrFZUQ9nWwv","colab_type":"code","outputId":"03b5f74c-2dcb-43cb-b471-2d8175413a02","executionInfo":{"status":"ok","timestamp":1585584996763,"user_tz":-120,"elapsed":716,"user":{"displayName":"brain pinky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSJ-MfHyLlQ17ipaf3IqBsqmMEcsDxuSrwhgT_Ng=s64","userId":"06438107502629971890"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["X_test, y_test = opp_sliding_window(X_test, y_test, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP_SHORT)\n","X_train, y_train = opp_sliding_window(X_train, y_train, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[9894, 24, 113]\n","[9894, 24]\n","[46495, 24, 113]\n","[46495, 24]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WlH9vxV7wvz1","colab_type":"code","colab":{}},"source":["np.savez_compressed('/content/drive/My Drive/Computer_vision_project/data/Opportunity/Opportunity_train_X.npz', X_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHBtHgiROCKF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"e7bfeece-43eb-49ec-ec28-b4ae10e4da85","executionInfo":{"status":"ok","timestamp":1585585031697,"user_tz":-120,"elapsed":1692,"user":{"displayName":"brain pinky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSJ-MfHyLlQ17ipaf3IqBsqmMEcsDxuSrwhgT_Ng=s64","userId":"06438107502629971890"}}},"source":["X_train.shape"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(46495, 24, 113)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"kqWwK8CWw-3F","colab_type":"code","colab":{}},"source":["np.savez_compressed('/content/drive/My Drive/Computer_vision_project/data/Opportunity/Opportunity_train_y.npz', y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ponMBw8vxBQ5","colab_type":"code","colab":{}},"source":["np.savez_compressed('/content/drive/My Drive/Computer_vision_project/data/Opportunity/Opportunity_test_X.npz', X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j9WOp7zUxCZp","colab_type":"code","colab":{}},"source":["np.savez_compressed('/content/drive/My Drive/Computer_vision_project/data/Opportunity/Opportunity_test_Y.npz', y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mo-PSy_sxPYp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}